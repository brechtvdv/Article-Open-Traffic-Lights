<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta charset="utf-8" />
  <title>Using an existing website as a queryable low-cost LOD publishing interface</title>
  <link rel="stylesheet" media="screen" href="styles/screen.css" />
  <link rel="stylesheet" media="print"  href="styles/print.css" />
  
</head>
<body prefix="rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# rdfs: http://www.w3.org/2000/01/rdf-schema# owl: http://www.w3.org/2002/07/owl# xsd: http://www.w3.org/2001/XMLSchema# dcterms: http://purl.org/dc/terms/ dctypes: http://purl.org/dc/dcmitype/ foaf: http://xmlns.com/foaf/0.1/ v: http://www.w3.org/2006/vcard/ns# pimspace: http://www.w3.org/ns/pim/space# cc: https://creativecommons.org/ns# skos: http://www.w3.org/2004/02/skos/core# prov: http://www.w3.org/ns/prov# qb: http://purl.org/linked-data/cube# schema: http://schema.org/ void: http://rdfs.org/ns/void# rsa: http://www.w3.org/ns/auth/rsa# cert: http://www.w3.org/ns/auth/cert# cal: http://www.w3.org/2002/12/cal/ical# wgs: http://www.w3.org/2003/01/geo/wgs84_pos# org: http://www.w3.org/ns/org# biblio: http://purl.org/net/biblio# bibo: http://purl.org/ontology/bibo/ book: http://purl.org/NET/book/vocab# ov: http://open.vocab.org/terms/ sioc: http://rdfs.org/sioc/ns# doap: http://usefulinc.com/ns/doap# dbr: http://dbpedia.org/resource/ dbp: http://dbpedia.org/property/ sio: http://semanticscience.org/resource/ opmw: http://www.opmw.org/ontology/ deo: http://purl.org/spar/deo/ doco: http://purl.org/spar/doco/ cito: http://purl.org/spar/cito/ fabio: http://purl.org/spar/fabio/ oa: http://www.w3.org/ns/oa# as: https://www.w3.org/ns/activitystreams# ldp: http://www.w3.org/ns/ldp# solid: http://www.w3.org/ns/solid/terms# acl: http://www.w3.org/ns/auth/acl# dio: https://w3id.org/dio# as: https://www.w3.org/ns/activitystreams# oa: http://www.w3.org/ns/oa# ldp: http://www.w3.org/ns/ldp#" typeof="schema:CreativeWork sioc:Post prov:Entity">
  <header>
  <h1 id="using-an-existing-website-as-a-queryable-low-cost-lod-publishing-interface">Using an existing website as a queryable low-cost LOD publishing interface</h1>

  <ul id="authors">
    <li><a href="https://brechtvdv.github.io/" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://w3id.org/people/brechtvdv/#me">Brecht Van de Vyvere</a></li>
    <li><a href="http://www.rubensworks.net/" typeof="http://xmlns.com/foaf/0.1/Person" resource="http://www.rubensworks.net/#me">Ruben Taelman</a></li>
    <li><a href="https://pietercolpaert.be" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://pietercolpaert.be/#me">Pieter Colpaert</a></li>
    <li><a href="https://ruben.verborgh.org/" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://ruben.verborgh.org/profile/#me">Ruben Verborgh</a></li>
  </ul>

  <ul id="affiliations">
    <li id="idlab">IDLab,
          Department of Electronics and Information Systems,
          Ghent University – imec</li>
  </ul>

  <section class="context">
    <h2 id="in-reply-to">In reply to</h2>
    <ul>
      <li><a href="https://www2019.thewebconf.org/call-for-demonstrations" rel="as:inReplyTo">The Web Conference 2019 Call for Demonstrations</a></li>
    </ul>
  </section>

</header>

<!-- Hack to make our custom fonts load in print-mode -->
<!-- https://stackoverflow.com/questions/39364259/chrome-print-preview-doesnt-load-media-only-print-font-face -->
<p><span class="printfont1"> </span>
<span class="printfont2"> </span>
<span class="printfont3"> </span>
<span class="printfont4"> </span></p>

<div class="double-column">

<section id="abstract">
    <h2>Abstract</h2>
    <!-- Context      -->
    <p>Maintaining an Open Dataset comes at an extra recurring cost when it is published in a dedicated Web interface.
<!-- Need         -->
As there is not often a direct financial return from publishing a dataset publicly, these extra costs need to be minimized.
<!-- Task         -->
Therefore we want to explore reusing existing infrastructure by enriching existing websites with Linked Data.
<!-- Object       -->
In this demonstrator, we advised the data owner to annotate a digital heritage website with JSON-LD snippets, resulting in a dataset of more than three million triples that is now available and officially maintained. <!--TODO: how much data was in te end published? Can we have some stats about the total data dump? -->
<!--Only an initial investment is required to have Linked Data snippets added to its corresponding webpages.-->
The website itself is paged, and thus hydra partial collection view controls were added in the snippets.
We then extended the modular query engine <a href="http://comunica.linkeddatafragments.org">Comunica</a> to support following page controls and extracting data from HTML documents while querying.
<!-- Findings     -->
This way, a SPARQL or GraphQL query over multiple heterogeneous data sources can power automated data reuse.
While the query performance on such an interface is visibly poor, it becomes easy to create composite data dumps.
<!-- Conclusion and Perspectives -->
As a result of implementing these building blocks in Comunica, any paged collection and enriched HTML page now becomes queryable by the query engine.
This enables heterogenous data interfaces to share functionality and become technically interoperable.</p>

  </section>


<main>
  <!-- Add sections by specifying their file name, excluding the '.md' suffix. -->
  <section id="introduction">
      <h2>Introduction</h2>

      <p>The <a href="https://viaa.be">Flemish Institute for Audiovisual Archiving</a> (VIAA) is a non-profit organization that preserves petabytes of valuable image, audio or video files. These files and accompanying metadata are covered by distinct <a href="https://viaa.be/nl/portaal/support-category/item/viaa-licenties-in-het-archiefsysteem">licenses</a>, but some can be made accessible under an Open Data <a href="https://creativecommons.org/publicdomain/zero/1.0/">license</a>. One initiative is opening up historical newspapers of the first World War with the open platform <a href="https://hetarchief.be">hetarchief.be</a>. In 2017, the raw data of these newspapers have been published as a <a property="schema:citation http://purl.org/spar/cito/cites" href="https://5stardata.info/en/">Linked Open Data</a> <span class="references">[<a href="#ref-1">1</a>]</span> (LOD) dataset using the low-cost <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">Triple Pattern Fragments</a></span> <span class="references">[<a href="#ref-2">2</a>]</span> (TPF) interface. Although this interface is still accessable (<a href="http://linkeddatafragments-qas.viaa.be/">http:/​/​linkeddatafragments-qas.viaa.be/</a>), no updates from the website have been exported to the TPF interface due to absence of automatisation.</p>

      <figure id="wiperstimes">
<center>
<img src="img/wiperstimes.PNG" />
</center>
<figcaption>
          <p><span class="label">Fig. 1:</span> The famous newspaper ‘Wipers Times’ published on 26th February 1916 (source: hetarchief.be).</p>
        </figcaption>
</figure>

      <p>Maintaining an up-to-date LOD interface brings besides technical resources also an organizational challenge. Content editors often work in a seperate environment such as a <a href="https://en.wikipedia.org/wiki/Content_management_system">Content Management System</a> (CMS) to update a website. The raw data gets exported from that system and published in a dedicated environment leaving the source of truth to the CMS. The question rises whether the data can be published closer to the authorative source in a more sustainable way.</p>

      <p>Website maintainers are currently using <a href="https://json-ld.org/spec/latest/json-ld/">JSON-LD</a> structured data snippets to attain better search result ranking and visualisation with search engines. These snippets are script tags inside a HTML webpage containing Linked Data in JSON format (JSON-LD) complaint with the Schema.org <span class="references">[<a href="#ref-3">3</a>]</span> datamodel. Not only <a href="https://support.google.com/webmasters/answer/3069489?hl=en">search engine optimization</a> (SEO), but also standard LOD publishing is possible. The data should be representative with the main content of the subject page, such as newspaper metadata, to be aligned with the structured data <a href="https://developers.google.com/search/docs/guides/sd-policies">guidelines</a> of search engines. Having n subject pages leads to n Linked Data Fragments that need to be linked together through hypermedia controls <span class="references">[<a href="#ref-4">4</a>]</span> so the website can be consumed as a dataset.</p>

      <p><span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1007/978-3-030-00668-6_15"><a href="https://comunica.github.io/Article-ISWC2018-Resource/">Comunica</a></span> <span class="references">[<a href="#ref-5">5</a>]</span> is a Linked Data user agent that can run federated queries over several heterogeneous Web APIs, such as data dumps, SPARQL-endpoints, Linked Data documents and Triple Pattern Fragments. This engine has been developed to make it easy to plug in specific types of functionality as separate modules. Such modules can be added or removed depending on the configuration. As such, by looking for affordances in Web APIs more intelligent user agents can be created.</p>

      <p>In this article, first we describe how hetarchief.be is enriched with JSON-LD snippets and how this enables federated querying with Comunica by adding two building blocks. After this, we demonstrate how a data dump can easily be generated. Finally, we discuss our conclusion.</p>

    </section>

  <section id="implementation">
      <h2>Implementation</h2>

      <h3 id="hetarchiefbe">hetarchief.be</h3>

      <p>Every newspaper webpage is annotated with JSON-LD snippets containing domain-specific metadata and hypermedia controls. The former metadata is described using acknowledged vocabularies such as <a href="http://dublincore.org/documents/dcmi-terms/">Dublin Core Terms</a> (DCTerms), <a href="http://xmlns.com/foaf/spec/">Friend of a Friend</a> (FOAF), <a href="https://schema.org/">Schema.org</a> etc. The latter is described using the <a href="https://www.hydra-cg.com/spec/latest/core">Hydra</a> vocabulary for hypermedia-driven Web APIs. Although hetarchief.be contains several human-readable hypermedia controls (free text search bar, search facets, pagination for every <a href="https://hetarchief.be/nl/media/brief-van-den-soldaat-aan-zijne-verdrukte-medeburgers/I2STYUAOpmFKmbFRXNmV0PTp">newspaper</a> ) only Hydras partial collection view controls are implemented: hydra:next describes the next newspaper, vice versa hydra:previous. Also an estimate of the amount of triples on a page is added using hydra:totalItems and void:triples. This helps user agents to build more efficient query plans.</p>

      <figure id="partial-collection-controls" class="listing">
<pre><code>{
</code><code>  &quot;@context&quot;: &quot;https://www.w3.org/ns/hydra/context.jsonld&quot;,
</code><code>  &quot;@id&quot;: &quot;https://hetarchief.be/media/de-school-op-het-front-studiebladen-van-sursum-corda/CMEPpOVIRqYiVZSYd3Q3k8tL&quot;,
</code><code>  &quot;previous&quot;: &quot;https://hetarchief.be/media/vrij-belgi%C3%AB/B1IVhaOMLFgCUGNJkVGuZH3S&quot;,
</code><code>  &quot;next&quot;: &quot;https://hetarchief.be/media/vrij-belgi%C3%AB/J1cnCMfndMbBNrde9VxIyVpB&quot;,
</code><code>  &quot;totalItems&quot;: 50,
</code><code>  &quot;http://rdfs.org/ns/void#triples&quot;: 50
</code><code>}</code></pre>
<figcaption>
          <p><span class="label">Listing 1:</span> Every newspaper describes its next and previous newspaper using Hydra partial collection view controls. This wires Linked Data Fragments together into a dataset.</p>
        </figcaption>
</figure>

      <p>In order to lower the barrier for automated Open Data reuse, information responses add the <a href="https://www.w3.org/TR/cors/">Cross-Origin Resource Sharing</a> (CORS) header: <em>Access-Control-Allow-Origin: *</em> . 
Not all metadata of a newspaper falls under an Open License. In the process of digitizing these newspapers, <a href="https://nl.wikipedia.org/wiki/Optical_character_recognition">Optical Character Recognition</a> (OCR) is applied. According to the <a href="https://eur-lex.europa.eu/eli/dir/2001/29/oj">European copyright legislation</a> content is still reserved by default 70 years after the death of the last author. This implies that these scanned texts cannot be published provisionally as Open Data. This is solved by publishing this content in a seperate Linked Data document covered by a different <a href="https://hetarchief.be/nl/gebruiksvoorwaarden">terms of use</a>. This also keeps the fragment size lean as such a <a href="https://hetarchief.be/nl/media/brief-van-den-soldaat-aan-zijne-verdrukte-medeburgers/I2STYUAOpmFKmbFRXNmV0PTp/ocr">document</a> measures easily 50 kilobytes for 4 newspaper pages.</p>

      <h3 id="building-blocks-comunica">Building blocks Comunica</h3>

      <p>To make Comunica work with hetarchief.be, two additional actors were needed.
First, we needed a generic actor to support pagination over any kind of hypermedia interface.
Secondly, an actor was needed to parse JSON-LD data snippets from HTML documents.
We will explain these two actors in more detail hereafter.</p>

      <p><code>BusRdfResolveHypermedia</code> is a bus in Comunica that resolves hypermedia controls from sources,
Currently, this bus only contains an actor that resolves controls for TPF interfaces.
We added a new actor (<code>ActorRdfResolveHypermediaNextPage</code>) to this bus that returns a search form containing a next page link, vice versa for previous page links.</p>

      <p>The parsing of most common Linked Data formats (<a href="https://www.w3.org/TR/turtle/">Turtle</a>, <a href="https://www.w3.org/TR/trig/">TriG</a>, <a href="https://www.w3.org/TR/rdf-syntax-grammar/">RDF/XML</a>, <a href="https://www.w3.org/2018/jsonld-cg-reports/json-ld/">JSON-LD</a>…) are already supported by Comunica.
However, no parser for extracting data snippets from HTML documents existed yet.
That is why we added an actor (<code>ActorRdfParseHtmlScript</code>) for parsing such HTML documents.
This intermediate parser searches for data snippets and forwards these to their respective RDF parser.
In case of a JSON-LD snippet, the body of a script tag  <code>&lt;script type="application/ld+json"&gt;</code> will be parsed by the JSON-LD parse actor.</p>

      <p>By adding these two actors to Comunica, we can now query over a paged collection that is declaratively described with data snippets. As federated querying comes out-of-the-box with Comunica, this cultural heritage collection can now be queried together with other knowledge bases (cfr. Wikidata). For example, <a href="#federated-querying-comunica">Listing 2</a> crawls over 17 newspaper pages. The first result appears after reading the first page. All results are available after 1,5 minutes. This is caused by deficiency of indexes where all pages need examination before having a complete answer.</p>

      <figure id="federated-querying-comunica" class="listing">
<pre><code>$ node packages/actor-init-sparql/bin/query.js
</code><code>hypermedia@https://hetarchief.be/nl/media/brief-van-den-soldaat-aan-zijne-verdrukte-medeburgers/r1ROcQtfXfhOZbUbUpivGRpY
</code><code>hypermedia@https://query.wikidata.org/bigdata/ldf
</code><code>&quot;SELECT * WHERE { 
</code><code>	GRAPH ?document {
</code><code>	 ?newspaper a &lt;http://schema.org/Newspaper&gt; . 
</code><code>	 ?newspaper &lt;http://schema.org/datePublished&gt; ?datePublished . 
</code><code>	 ?newspaper &lt;http://purl.org/dc/terms/title&gt; ?title . 
</code><code>	 ?newspaper &lt;http://schema.org/publisher&gt; ?publisher . 
</code><code> 	}
</code><code>}&quot;</code></pre>
<figcaption>
          <p><span class="label">Listing 2:</span> SPARQL-query over a paged collection of hetarchief.be and the TPF interface of Wikidata using the JavaScript-based command line interface of Comunica.</p>
        </figcaption>
</figure>

      <p>In next section we will demonstrate how SPARQL-querying can be applied for extracting a spreadsheet.</p>
    </section>

  <section id="demonstrator">
      <h2>Demonstrator</h2>

      <p>This demonstrator shows that a non-technical user can create a data dump from the cultural heritage website hetarchief.be. More specifically, a spreadsheet can be extracted using SPARQL-querying from embedded paged collection views.
The application is written with the front-end playground Codepen <a href="https://codepen.io/brechtvdv/pen/ebOzXB">https:/​/​codepen.io/brechtvdv/pen/ebOzXB</a>. A browser compatible library of Comunica is built  using a custom configuration that can be found on Github (<a href="https://github.com/brechtvdv/hetarchief-comunica">https:/​/​github.com/brechtvdv/hetarchief-comunica</a>) under an Open License.</p>

      <figure id="codepen">
<center>
<img src="img/codepen.PNG" />
</center>
<figcaption>
          <p><span class="label">Fig. 2:</span> A spreadsheet is generated by entering a URL of a newspaper from hetarchief.be.</p>
        </figcaption>
</figure>

      <p>First, a user can insert a URL of a hypermedia-enabled LOD interface. For example, a user can go to <a href="https://hetarchief.be/nl/zoeken/%2A?Filetype%5Bdocument%5D=document">hetarchief.be</a> and select a newspaper as starting point.
After pressing <em>Start downloading</em>, Comunica starts fetching the entrypoint and follows the embedded pagination controls. During querying, user feedback is provided with the amount of processed CSV rows and bytes.
Next, the user can <em>Copy</em> the CSV output to its clipboard and save it using spreadsheet software.
Optionally, a SPARQL-query can be configured to customize the desired outcome.</p>

      <p data-height="542" data-theme-id="0" data-slug-hash="ebOzXB" data-default-tab="result" data-user="brechtvdv" data-pen-title="Converteren van website naar spreadsheet" class="codepen">See the Pen <a href="https://codepen.io/brechtvdv/pen/ebOzXB/">Download your website as a spreadsheet
</a> by Brecht Van de Vyvere (<a href="https://codepen.io/brechtvdv">@brechtvdv</a>) on <a href="https://codepen.io">CodePen</a>.</p>
      <script async="" src="https://static.codepen.io/assets/embed/ei.js"></script>

    </section>

  <section id="sota">
      <h2>Background</h2>

      <h3 id="comunica">Comunica</h3>

      <p>Every piece of functionality in Comunica can be implemented as seperate building blocks based on the <em>actor</em> programming model, where each actor can respond to a specific action. Actors that share same functionality, but with different implementations, can be grouped with a communication channel called a <em>bus</em>. Interaction between actors is possible through a <em>mediator</em> that wraps around a bus to get an action’s result from a single actor. This result depends on the configuration of the mediator, e.g. a race mediator will return the response of the actor that is able to reply the earliest.</p>

      <figure id="actor">
<center>
<img src="img/actor-mediator-bus.svg" />
</center>
<figcaption>
          <p><span class="label">Fig. 3:</span> Actor 0 initiates an action to a mediator. The mediator communicates through a bus with all actors 1, 2 and 3 that are able to solve the action and gives back the most favorable result according to its configuration.</p>
        </figcaption>
</figure>

      <h3 id="hydra-partial-collection-views">Hydra partial collection views</h3>

      <p>Open Data is filled with collections of members (hotel ammenities, road works etc.). These related resources can be grouped as members of a collection using the Hydra vocabulary. When the size of mebers is too big, data owners can fragment this into collection views. Each view represents a part of the collection to keep the Web API responses lightweight. In the case of hetarchief.be represents each newspaper HTTP document one view of the collection of newspapers. These views are linked together with partial collection view controls: previous, next, first and last. This allows a client to fetch all members of the collection.</p>
    </section>

  <section id="conclusion">
      <h2>Conclusion</h2>

      <p>Data owners can publish their Linked Open Data very cost-efficient on their website with JSON-LD snippets. After an initial cost of adding this feature to their website, they can have an always up-to-date dataset with negligible maintenance costs. The cultural heritage website hetarchief.be showcases an official maintained paged collection of Linked Data Fragments about newspapers. By extending Comunica, in-depth data analyzation and federated querying over this dataset is possible. To improve querying speed, Linked Data services (<a href="http://semanticweb.org/wiki/SPARQL_endpoint.html">SPARQL-endpoint</a>, HDT <span class="references">[<a href="#ref-6">6</a>]</span>, TPF interface…) with a higher maintenance cost can be created on top of JSON-LD snippets. Such interfaces would suffer from scalability problems: Optical Character Recognition (OCR) texts have bad compression rates, and thus require gigabytes of disk space. With our solution, these OCR-text are published in a seperate document keeping the maintenance cost low while harvesting in an automated way is still possible. By using our demonstrator, non-technical users can extract a data dump from an enriched website.</p>

      <p>To gain traction with an international audience, e.g. the science stories platform (http://sciencestories.io), a reconciliation service could be created with knowledge bases (cfr. Wikidata). 
Next to embedding the data, hypermedia controls or search engine optimization features, also the <a href="https://iiif.io/api/image/2.1/">International Image Interoperability Framework</a> (IIIF) Image API for sharing images could be described within a JSON-LD snippet for raising the discoverability of this service. IIIF API information already uses JSON-LD to describe its features such as tiling and licensing which makes this an excellent snippet addition helping an organization become more visible on the Web.</p>

      <p>In future work, extending Comunica for harvesting Hydra collections would help organizations to improve their collection management. These collections could be defined on their main page of their website improving Open Data discoverability. Also work on supporting multiple views acting as indexes for collections would benefit querying performance on sorting or filtering operations on e.g. geospatial or temporal data.</p>
    </section>

</main>

<footer>
<section id="references">
<h2>References</h2>
<dl class="references">
  <dt id="ref-1">[1]</dt>
  <dd resource="https://5stardata.info/en/" typeof="schema:CreativeWork">Berners-Lee, T.: 5 Star Data. <a href="https://5stardata.info/en/">https:/​/​5stardata.info/en/</a> (2009).</dd>
  <dt id="ref-2">[2]</dt>
  <dd resource="https://dx.doi.org/10.1016/j.websem.2016.03.003" typeof="schema:Article">Verborgh, R., Vander Sande, M., Hartig, O., Van Herwegen, J., De Vocht, L., De Meester, B., Haesendonck, G., Colpaert, P.: Triple Pattern Fragments: a Low-cost Knowledge Graph Interface for the Web. Journal of Web Semantics. 37–38, (2016).</dd>
  <dt id="ref-3">[3]</dt>
  <dd resource="https://dx.doi.org/10.1109/MIC.2015.81" typeof="schema:Article">Mika, P.: On Schema.org and Why It Matters for the Web. IEEE Internet Computing. 19, 52–55 (2015).</dd>
  <dt id="ref-4">[4]</dt>
  <dd resource="#fielding2000architectural" typeof="schema:Book">Fielding, R.T., Taylor, R.N.: Architectural styles and the design of network-based software architectures. University of California, Irvine Irvine, USA (2000).</dd>
  <dt id="ref-5">[5]</dt>
  <dd resource="https://dx.doi.org/10.1007/978-3-030-00668-6_15" typeof="schema:Article">Taelman, R., Van Herwegen, J., Vander Sande, M., Verborgh, R.: Comunica: a Modular SPARQL Query Engine for the Web. In: Vrandečić, D., Bontcheva, K., Suárez-Figueroa, M.C., Presutti, V., Celino, I., Sabou, M., Kaffee, L.-A., and Simperl, E. (eds.) Proceedings of the 17th International Semantic Web Conference. pp. 239–255. Springer (2018).</dd>
  <dt id="ref-6">[6]</dt>
  <dd resource="#Fernndez2013BinaryRR" typeof="schema:Article">Fernández, J.D., Martínez-Prieto, M.A., Gutiérrez, C., Polleres, A., Arias, M.: Binary RDF representation for publication and exchange (HDT). J. Web Sem. 19, 22–41 (2013).</dd>
</dl>
</section>
</footer>

</div>



</body>
</html>
