## Introduction
{:#introduction}

The [Flemish Institute for Audiovisual Archiving](https://viaa.be) (VIAA) is a non-profit organization that preserves petabytes of valuable image, audio or video files. These files and accompanying metadata are covered by distinct [licenses](https://viaa.be/nl/portaal/support-category/item/viaa-licenties-in-het-archiefsysteem), but some can be made accessible under an Open Data [license](https://creativecommons.org/publicdomain/zero/1.0/). One initiative is opening up historical newspapers of the first World War with the open platform [hetarchief.be](https://hetarchief.be). In 2017, the raw data of these newspapers have been published as a [Linked Open Data](cite:cites 5stars) (LOD) dataset using the low-cost [Triple Pattern Fragments](cite:cites tpf) (TPF) interface. Although this interface is still accessable ([http://linkeddatafragments-qas.viaa.be/](http://linkeddatafragments-qas.viaa.be/)), no updates from the website have been exported to the TPF interface due to absence of automatisation.

Maintaining an up-to-date LOD interface brings besides technical resources also an organizational challenge. Content editors often work in a seperate environment such as a [Content Management System](https://en.wikipedia.org/wiki/Content_management_system) (CMS) to update a website. The raw data gets exported from that system and published in a dedicated environment leaving the source of truth to the CMS. The question rises whether the data can be published closer to the authorative source in a more sustainable way.

Website maintainers are currently using [JSON-LD](https://json-ld.org/spec/latest/json-ld/) structured data snippets to attain better search result ranking and visualisation with search engines. These snippets are script tags inside a HTML webpage containing Linked Data in JSON format (JSON-LD) conform the [Schema.org](cite:cites 7131410) datamodel. Not only [search engine optimalization](https://support.google.com/webmasters/answer/3069489?hl=en) (SEO), but also standard LOD publishing is possible. The data should be representative with the main content of the subject page, such as newspaper metadata, to be aligned with the structured data [guidelines](https://developers.google.com/search/docs/guides/sd-policies) of search engines. Having n subject pages leads to n Linked Data Fragments that need to be linked together through hypermedia [controls](cite:cites fielding2000architectural) so the website can be consumed as a dataset.

[Comunica](cite:cites taelman_iswc_2018) is a Linked Data user agent that can run federated queries over several heterogeneous Web APIs, such as data dumps, SPARQL-endpoints, Linked Data documents and Triple Pattern Fragments. This engine has been developed to make it easy to plug in specific types of functionality as separate modules. Such modules can be added or removed depending on the configuration. As such, this allows hypermedia driven clients to be created.

In this article, first we describe how hetarchief.be is enriched with JSON-LD snippets and how this enables federated querying with Comunica by adding two building blocks. After this, we demonstrate how a data dump can easily be generated. Finally, we discuss our conclusion.

